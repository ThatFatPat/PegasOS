/**
 * @addtogroup x86_64
 * @{
 * @file control_regs.h Helpers for manipulating control registers in x86_64
 * CPUs.
 */

#pragma once

// Control registers

/**
 * Enable protected mode.
 */
#define X86_64_CR0_PE (1 << 0)

/**
 * Enable kernel write protection. With this flag set, any attempt by the kernel
 * to write to read-only memory will cause a page fault, like in user mode.
 */
#define X86_64_CR0_WP (1 << 16)

/**
 * Enable paging.
 */
#define X86_64_CR0_PG (1 << 31)

/**
 * Enable physical address extensions.
 */
#define X86_64_CR4_PAE (1 << 5)

/**
 * Indicate OS support for the `fxsave` instruction, which saves SSE registers.
 * Without this flag set, any attempt to use an SSE instruction will result in
 * an undefined opcode exception.
 */
#define X86_64_CR4_OSFXSR (1 << 9)

/**
 * Indicate OS support for unmasked floating-point SSE exceptions. Without this
 * flag set, any floating-point exception generated by an SSE instruction will
 * result in an undefined opcode exception instead.
 */
#define X86_64_CR4_OXMMEXCPT (1 << 10)


// MSRs

/**
 * Register number of `IA32_EFER` MSR. This MSR controls various processor
 * features, including long mode and the NX bit. For details, see Intel 3a
 * Section 2.2.1.
 */
#define X86_64_MSR_EFER 0xC0000080

/**
 * Enable the `syscall` instruction.
 */
#define X86_64_MSR_EFER_SCE (1 << 0)

/**
 * Enable long mode.
 */
#define X86_64_MSR_EFER_LME (1 << 8)

/**
 * Enable use of the NX bit in paging structures. With this flag set, any paging
 * structure entry marked with @ref X86_64_MMU_FLAG_NX will not be executable.
 */
#define X86_64_MSR_EFER_NXE (1 << 11)

#ifndef __ASSEMBLER__

#include <stdint.h>

namespace arch::x86_64 {

/**
 * Read CR0 (processor state control flags).
 * @return Current value of CR0.
 */
inline uint64_t read_cr0() {
  uint64_t ret;
  asm volatile("mov %%cr0, %0" : "=r"(ret));
  return ret;
}

/**
 * Update CR0 (processor state control flags).
 * @param value New value of CR0.
 */
inline void write_cr0(uint64_t value) {
  asm volatile("mov %0, %%cr0" : : "r"(value));
}

/**
 * Read CR2 (page fault linear address).
 * @return Current value of CR2.
 */
inline uint64_t read_cr2() {
  uint64_t ret;
  asm volatile("mov %%cr2, %0" : "=r"(ret));
  return ret;
}

/**
 * Read CR3 (global page table pointer).
 * @return Current value of CR3.
 */
inline uint64_t read_cr3() {
  uint64_t ret;
  asm volatile("mov %%cr3, %0" : "=r"(ret));
  return ret;
}

/**
 * Update CR3 (global page table pointer).
 * @param value New value of CR3.
 */
inline void write_cr3(uint64_t value) {
  asm volatile("mov %0, %%cr3" : : "r"(value) : "memory");
}

/**
 * Read CR4 (architectural extensions and processor capabilities).
 * @return Current value of CR4.
 */
inline uint64_t read_cr4() {
  uint64_t ret;
  asm volatile("mov %%cr4, %0" : "=r"(ret));
  return ret;
}

/**
 * Update CR4 (architectural extensions and processor capabilities).
 * @param value New value of CR4.
 */
inline void write_cr4(uint64_t value) {
  asm volatile("mov %0, %%cr4" : : "r"(value));
}

/**
 * Read a model-specific register.
 * @param id The ID of the model-pecific register to read.
 * @return Current state of the MSR specified by `id`.
 */
inline uint64_t read_msr(uint32_t id) {
  uint32_t low;
  uint32_t high;
  asm volatile("rdmsr" : "=d"(high), "=a"(low) : "c"(id));
  return static_cast<uint64_t>(high) << 32 | low;
}

/**
 * Write to a model-specific register.
 * @param id The ID of the model-specific register to write.
 * @param value The value to be stored.
 */
inline void write_msr(uint32_t id, uint64_t value) {
  auto low = static_cast<uint32_t>(value);
  auto high = static_cast<uint32_t>(value >> 32);

  asm volatile("wrmsr" : : "c"(id), "d"(high), "a"(low));
}

} // namespace arch::x86_64

#endif

/** @} */
